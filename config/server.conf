[app]
# Application settings
name = "veil-armor"
version = "1.0.0"
host = "0.0.0.0"
port = 5000
debug = false
workers = 4

[security]
# Security settings
api_key_enabled = false
api_key = ""
cors_enabled = true
cors_origins = ["*"]
rate_limit_enabled = true
rate_limit_per_minute = 60

[scanners]
# Enable/disable individual scanners
vectordb_enabled = true
yara_enabled = true
transformer_enabled = true
similarity_enabled = true
sentiment_enabled = false
relevance_enabled = false

# Scanner weights for risk scoring (must sum to 1.0)
vectordb_weight = 0.35
yara_weight = 0.30
transformer_weight = 0.35

[vectordb]
# Vector database configuration
type = "chromadb"  # chromadb, faiss, or pinecone
path = "data/vectordb"
collection_name = "veil_armor_threats"

# Embedding model settings
# Options: 
#   - sentence-transformers/all-MiniLM-L6-v2 (fast, local)
#   - sentence-transformers/all-mpnet-base-v2 (better quality, local)
#   - text-embedding-ada-002 (OpenAI, requires API key)
embedding_model = "sentence-transformers/all-MiniLM-L6-v2"
embedding_dimension = 384

# Detection thresholds
similarity_threshold = 0.85  # Lower = more sensitive (0.0-1.0)
top_k = 5  # Number of similar matches to retrieve

# Auto-update vector DB with detected threats
auto_update_enabled = true
auto_update_threshold = 0.95  # Only add high-confidence detections

[transformer]
# Transformer model configuration
model_name = "deepset/deberta-v3-base-injection"
# Alternative models:
#   - ProtectAI/deberta-v3-base-prompt-injection-v2
#   - jackaduma/SecRoBERTa

device = "cpu"  # cpu or cuda
threshold = 0.98  # Detection threshold (0.0-1.0)
max_length = 512

[yara]
# YARA rules configuration
rules_path = "data/yara_rules"
compiled_rules_path = "data/yara_rules/compiled_rules.yarc"
auto_compile = true

# Rule categories to enable
categories = [
    "instruction_bypass",
    "jailbreak",
    "data_exfiltration",
    "goal_hijacking",
    "prompt_leakage"
]

[similarity]
# Prompt-response similarity scanner
enabled = true
threshold = 0.15  # Minimum similarity threshold
model = "sentence-transformers/all-MiniLM-L6-v2"

[sentiment]
# Sentiment analysis scanner
enabled = false
model = "cardiffnlp/twitter-roberta-base-sentiment"
suspicious_threshold = 0.85

[canary]
# Canary token configuration
enabled = true
default_length = 16
default_header = "<-@!-- {canary} --@!->"
storage_path = "data/canary_tokens.db"

# Canary patterns
patterns = [
    "<-@!--.*?--@!->",
    "\\[CANARY:.*?\\]",
    "<<<.*?>>>"
]

[openai]
# OpenAI configuration (optional)
enabled = false
api_key = ""  # Set via environment variable OPENAI_API_KEY
model = "gpt-3.5-turbo"
embedding_model = "text-embedding-ada-002"

[logging]
# Logging configuration
level = "INFO"  # DEBUG, INFO, WARNING, ERROR, CRITICAL
format = "json"  # json or text
file = "logs/veil_armor.log"
max_file_size_mb = 100
backup_count = 5

# Log detections
log_detections = true
log_path = "logs/detections.jsonl"

[monitoring]
# Monitoring and metrics
prometheus_enabled = false
prometheus_port = 9090
metrics_path = "/metrics"

[datasets]
# Dataset configuration for loading
instruction_bypass_dataset = "deadbits/vigil-instruction-bypass-ada-002"
jailbreak_dataset = "deadbits/vigil-jailbreak-ada-002"

# Custom datasets
custom_datasets = [
    # "path/to/custom_dataset.json"
]

[performance]
# Performance settings
batch_size = 32
max_concurrent_requests = 100
timeout_seconds = 30
cache_enabled = true
cache_ttl_seconds = 3600

[response]
# Response configuration
include_explanations = true
include_scanner_details = true
sanitize_output = false
default_safe_response = "I cannot process this request."
